{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHX-RCPdkXgP",
    "outputId": "d294f833-6f45-4014-e4ed-75ef26141a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  4 19:45:55 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti      On | 00000000:26:00.0 Off |                  N/A |\n",
      "|  0%   54C    P8               15W / 200W|     15MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1347      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1559      G   /usr/bin/gnome-shell                          3MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuXIFTFapAMI",
    "outputId": "340f0ca7-6dfd-4350-b2d2-15f983adfc66",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets\n",
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "First let's load the model we are going to use - GPT-neo-x-20B! Note that the model itself is around 40GB in half precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "278070bfaaac4794a649f11306ad2947",
      "5391d6d2385549319099d3bf653c34ee",
      "ee696f00718b43989f653dd01391a87d",
      "e39e4f5284224bfdbde4359d8593d206",
      "0a91d94890634dc1bb43176347c27fec",
      "ec9d84f9337948cba5562b7e995bb2ef",
      "3473e65596d5455385269f59e6159e38",
      "416d78e5558f4019972720c8d52a410b",
      "388757e992344b06b2fc7e4573eba77d",
      "664f6a3720a446929197650580dd005d",
      "92e0594143bd4c2690dc3ddaa0170b17"
     ]
    },
    "id": "jm4FzCvfeYcK",
    "outputId": "bf40f8d2-7603-407d-fba5-6686eddd49aa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/issac/anaconda3/envs/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset json (/home/issac/.cache/huggingface/datasets/json/default-95ad2bd7d3162001/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 836.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"squarelike/ko_medical_chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2KUhV7x3e6Db",
    "outputId": "1fdf62ba-5269-435c-dd11-29a381ba04c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'conversations'],\n",
       "        num_rows: 3038\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FbgsI9sezTJ",
    "outputId": "7df84efa-917f-4d13-e5ba-62fc1d0b3d25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/issac/.cache/huggingface/datasets/json/default-95ad2bd7d3162001/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ba47b88b4c6a0c5e.arrow\n"
     ]
    }
   ],
   "source": [
    "data = data.map(\n",
    "    lambda x: {\n",
    "        'text': \"\\n\".join([f\"{'환자' if line['from']=='client' else '의사'}: {line['value']}{'</끝>' if line['from']!='client' else ''}\" for line in x['conversations']])\n",
    "      }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "e601d45a-99f5-46ff-aecf-497ed8754c24",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"squarelike/llama2-ko-medical-7b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4TDUgDbhyhK",
    "outputId": "8aac9a8c-5211-4cb8-bd43-3b99b24c9c00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/issac/.cache/huggingface/datasets/json/default-95ad2bd7d3162001/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-714327b44749993e.arrow\n"
     ]
    }
   ],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"text\"], truncation=True, max_length=2048), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'환자: 밤에 자러 갈 때 허벅지 중앙을 따라 지속적인 통증이 있어요. 혈전이 있는지 검사를 받아봤는데 없었어요. 또한 등 아래쪽에 관절염이 있다는 결과가 나왔어요. 때로는 오른쪽으로 누울 수 없을 정도로 통증이 있어요. 그 통증은 허벅지에 있어요. 주로 자러 갈 때에만 나타나요.\\n의사: 허리 통증과 허벅지로 전이되는 것은 싸이아티카로 해석될 수 있는데, 싸이아티카는 신경이 경로 상에서 압박을 받아 통증이 발생하는 것입니다. 보존적으로 치료하기 위해 딱딱한 침대에서 누우며 무릎 아래에 작은 쿠션을 놓는 것은 신경에 가해지는 압력을 줄여주어 통증을 완화시킬 것입니다. 여행 중에는 요추 보호대를 사용하여 추가적인 외상을 방지하고, 혈액 칼슘, 비타민 D 및 비타민 B12 수치를 점검해보세요. 이러한 수치가 정상보다 낮다면 상태를 악화시킬 수 있습니다. 일이 여행을 요구하는 경우 가벼운 짐을 가지세요. 식단에는 녹색 야채와 액체를 많이 섭취하고, 비타민 B12 보충제를 복용하세요(비타민 B12는 신경 토닉입니다). 가벼운 운동과 저출력 에어로빅(자전거 타기 및 수영)을 하고, 올바른 자세와 작업 환경을 유지하세요(잘못된 자세는 상태를 악화시킵니다). 통증이 주로 자러 갈 때 발생한다면 딱딱한 침대에서 자거나 반인치 메모리 폼 매트리스로</끝>\\n환자: 자려고 할 때 통증이 주로 발생하는데, 어떻게 대처해야 할까요?\\n의사: 통증을 완화시키기 위해 딱딱한 침대에서 누우며 무릎 아래에 작은 쿠션을 놓는 것이 좋습니다. 또한 혈액 검사를 통해 칼슘, 비타민 D 및 비타민 B12 수치를 확인하고 보충제를 복용하는 것이 도움이 될 수 있습니다. 가벼운 운동과 올바른 자세 유지도 중요합니다.</끝>\\n환자: 그렇군요. 감사합니다.\\n의사: 언제든지 도움이 필요하면 말씀해주세요. 건강하게 지내세요.</끝>\\n환자: 감사합니다. 안녕히 계세요.\\n의사: 안녕히 가세요.</끝>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][1000]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ybeyl20n3dYH",
    "outputId": "327a4a63-db3d-4631-edec-23f8d08d14ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3670016 || all params: 3070156800 || trainable%: 0.11953838970048696\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCc64bfnmd3j"
   },
   "source": [
    "Let's load a common dataset, english quotes, to fine tune our model on famous quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenNum_ai = 33687     # \"의사\"\n",
    "tokenNum_human = 35604   # \"환자\"\n",
    "tokenNum_com = 29901        # \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "class maskTrainer(Trainer):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    for x in range(len(inputs['labels'])):\n",
    "      # print(tokenizer.decode(inputs['labels'][x]))\n",
    "\n",
    "      maskindex1 = (inputs['labels'][x]==tokenNum_human).nonzero()[:, 0].cpu()\n",
    "      temp = 0\n",
    "      for i, index in enumerate(maskindex1):\n",
    "        if (inputs['labels'][x][index+1] != tokenNum_com):\n",
    "          maskindex1 = np.delete(maskindex1, i-temp)\n",
    "          temp += 1\n",
    "\n",
    "      maskindex2 = (inputs['labels'][x]==tokenNum_ai).nonzero()[:, 0].cpu()\n",
    "      temp = 0\n",
    "      for i, index in enumerate(maskindex2):\n",
    "        if (inputs['labels'][x][index+1] != tokenNum_com):\n",
    "          maskindex2 = np.delete(maskindex2, i-temp)\n",
    "          temp += 1\n",
    "\n",
    "      for i in range(len(maskindex1)):\n",
    "        ai_index = -1\n",
    "        for num in maskindex2:\n",
    "          if (maskindex1[i] < num):\n",
    "            ai_index = num\n",
    "            break\n",
    "        if (ai_index == -1):\n",
    "          inputs['labels'][x][maskindex1[i]+2:] = -100\n",
    "        else:\n",
    "          inputs['labels'][x][maskindex1[i]+2:ai_index+2] = -100\n",
    "    # print(inputs['labels'][x])\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    return (loss,outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jq0nX33BmfaC",
    "outputId": "769fb34e-acd0-4a56-ac87-40db98dabd78",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/issac/anaconda3/envs/jupyter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='206' max='15190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  206/15190 04:09 < 5:04:52, 0.82 it/s, Epoch 0.07/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.499500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import transformers\n",
    "\n",
    "# # needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = maskTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # warmup_steps=200,\n",
    "        #max_steps=3000, ## 초소형만 학습: 10 step = 20개 샘플만 학습.\n",
    "        fp16=True,\n",
    "        output_dir=\"outputs\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=50,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        num_train_epochs = 5,\n",
    "        learning_rate=3e-4,\n",
    "        # resume_from_checkpoint=\"./outputs/checkpoint-9500\",\n",
    "        # resume_from_checkpoint=True,\n",
    "        lr_scheduler_type= \"cosine\",\n",
    "        #optim=\"paged_adamw_8bit\"\n",
    "    \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-jauOEv9XVe"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved/doctor/7B/try2_5epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "stop_words = [\"</끝>\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"아래는 전문적인 의사와 환자의 진료 기록이다.\n",
    "\n",
    "환자: 갑자기 무릎이 아파요.\n",
    "의사: 언제부터 그런 증상이 있었나요?</끝>\n",
    "환자: 조금 전 부터에요. 전에 이 부위가 골절된 적이 있었는데. 지금은 전부 치료되었거든요. 왜 또 아플까요?\n",
    "의사: 무릎을 움직일 때 통증이 심해지나요?</끝>\n",
    "환자: 네. 무릎에 힘을 주면 깨질듯이 아파요.\n",
    "의사: 무릎을 움직이는 것은 가능한가요?</끝>\n",
    "환자: 음...못 움직입니다. 너무 아파요\n",
    "의사:\"\"\"\n",
    "\n",
    "tokenizer.decode(model.generate(\n",
    "    **tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt', \n",
    "        return_token_type_ids=False\n",
    "    ),\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.2,\n",
    "    no_repeat_ngram_size=10,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=2,\n",
    "    stopping_criteria=stopping_criteria\n",
    ")[0]).replace(prompt+\" \", \"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a91d94890634dc1bb43176347c27fec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "278070bfaaac4794a649f11306ad2947": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5391d6d2385549319099d3bf653c34ee",
       "IPY_MODEL_ee696f00718b43989f653dd01391a87d",
       "IPY_MODEL_e39e4f5284224bfdbde4359d8593d206"
      ],
      "layout": "IPY_MODEL_0a91d94890634dc1bb43176347c27fec"
     }
    },
    "3473e65596d5455385269f59e6159e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "388757e992344b06b2fc7e4573eba77d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "416d78e5558f4019972720c8d52a410b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5391d6d2385549319099d3bf653c34ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec9d84f9337948cba5562b7e995bb2ef",
      "placeholder": "​",
      "style": "IPY_MODEL_3473e65596d5455385269f59e6159e38",
      "value": "100%"
     }
    },
    "664f6a3720a446929197650580dd005d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92e0594143bd4c2690dc3ddaa0170b17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e39e4f5284224bfdbde4359d8593d206": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_664f6a3720a446929197650580dd005d",
      "placeholder": "​",
      "style": "IPY_MODEL_92e0594143bd4c2690dc3ddaa0170b17",
      "value": " 1/1 [00:00&lt;00:00,  9.41it/s]"
     }
    },
    "ec9d84f9337948cba5562b7e995bb2ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee696f00718b43989f653dd01391a87d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_416d78e5558f4019972720c8d52a410b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_388757e992344b06b2fc7e4573eba77d",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
